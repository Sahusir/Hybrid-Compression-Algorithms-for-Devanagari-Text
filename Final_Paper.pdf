PERFORMANCE EVALUATION OF EFFICIENT HYBRID COMPRESSION METHODS FOR DEVANAGARI-ENCODED HINDI TEXT USING LOSSLESS ALGORITHMS
•	Mukesh Sahu Delhi Technological University, Delhi 110 042, India
•	Jeebananda Panda Delhi Technological University, Delhi 110 042, India
Abstract. This  research paper provides a comprehensive performance analysis of five standard lossless compression algorithms—LZMA, Zstd, Brotli, Bzip2, and LZ4HC, —alongside sixty hybrid combinations on UTF-8 encoded Hindi text datasets, written in the Devanagari script, across varying sizes—small, medium, and large. The evaluation covers compression ratio, compression speed, decompression speed, and a derived weighted normalized efficiency score. Hybrid algorithms—especially those combining Zstd with LZ4HC or Brotli—demonstrate superior performance across all individual parameters.  Independent Zstd achieves the highest efficiency score for the medium-sized dataset, illustrating how a well-balanced algorithm can excel in overall efficiency despite not leading in any single metric. However, in terms of compression ratio, speed, and decompression speed, hybrid methods consistently outperform standalone approaches. Notably, the hybrid combination Zstd + LZ4HC achieves the highest efficiency, with scores of 0.6764 for small files and 0.8597 for large files. Independent algorithms such as Zstd and Bzip2 perform well but are consistently surpassed by strategic hybrid pairings. The analysis underscores the significance of combining fast and high-ratio compression techniques, particularly for language-specific data. However, not all hybrids yielded significant improvements, with some introducing computational overhead. This comparative analysis highlights the potential of hybrid compression in applications such as cloud storage, mobile messaging, and NLP-based Hindi text processing.
Keywords:
Hindi text compression, lossless compression, hybrid algorithms, Zstd, LZ4HC, LZMA Brotli, Bzip2, compression ratio, decompression speed, performance evaluation.
1. INTRODUCTION
Data Compression is an important issue for digital communication system. Data as it is essential for minimizing storage requirements and enhancing transmission efficiency, particularly for languages like Hindi. Hindi text, encoded using the Devanagari script and represented in Unicode, typically results in larger file sizes due to the presence of multi-byte characters. These characteristics lead to irregular character frequency patterns, making conventional compression algorithms less effective.
Given Hindi’s widespread usage across government, education, and media sectors, developing effective compression methods is crucial—especially in regions with bandwidth or storage constraints. Better compression directly improves data accessibility, lowers infrastructure costs, and supports the broader dissemination of Hindi content in digital form.
This study focuses on hybrid compression techniques that apply two lossless algorithms sequentially. Specifically, we examine the following five algorithms: LZMA (Lempel-Ziv-Markov chain algorithm), Zstd (Zstandard), Brotli, Bzip2, and LZ4HC (High Compression variant of LZ4). These algorithms were selected due to their proven performance in text compression and their ability to balance high compression ratios with fast processing speeds.
•	Zstd is recognized for its fast compression and decompression speeds, making it suitable for real-time applications.[1]
•	Brotli, developed by Google, has demonstrated strong performance in web compression, providing a balance between speed and compression ratio.[2]
•	. LZMA, known for its high compression ratio, has been widely used in applications where space-saving is critical, particularly for archiving and large dataset[3][4]
•	LZ4HC is a high-compression variant of LZ4, providing fast decompression speeds while achieving a higher compression ratio, suitable for scenarios where quick access to compressed data is required.[5]
•	Bzip2, with its Burrows-Wheeler transform (BWT), is known for its excellent compression ratio but at the cost of slower compression speeds, making it ideal for cases where compression efficiency is prioritized over speed.[6]
By combining these algorithms in hybrid configurations, we aim to leverage the strengths of each, addressing the unique challenges posed by the Devanagari script. This approach not only seeks to achieve efficient compression but also ensures fast processing times, making it feasible for real-world applications where both storage and speed are of paramount importance.
2. LITERATURE REVIEW
The literature reveals extensive research in the field of lossless text compression, with focus areas ranging from standard algorithm evaluations to language-specific and hybrid approaches.
2.1. Standard Compression Algorithms:
Salomon and Motta [7] provide a foundational overview of data compression algorithms, highlighting trade-offs in compression ratio, speed, and complexity—establishing theoretical grounding for hybrid methods. Aruna et al. [8] review classical algorithms like Huffman and LZ77, including their hybrid form Deflate, underlining their relevance in cloud computing. Gap: Their study omits analysis on Indic scripts and does not explore language-specific adaptations. Gupta et al. [9] compare Deflate, Bzip2, LZMA, PPMd, and PPMonstr using the Silesia corpus. Gap: The evaluation is limited to English and general-purpose datasets, with no insight into language-based optimization.
2.2. Hybrid Compression Techniques:
Tariq et al. [10] propose serial combinations of algorithms to improve compression outcomes. Gap: The paper lacks benchmarking on Unicode-based or non-English language corpora.
2.3. Hindi Text Compression:
Gupta et al. [11] address Devanagari composite character compression using hybrid Huffman and run-length coding, achieving up to 67% savings. Gap: The work is limited to font pattern compression and not generalized for full-text Hindi datasets.
2.4. Indic Language Compression:
Vijayalakshmi et al. [12] designed a Tamil text compression method outperforming Huffman, ZIP, and LZW. Gap: Their method is specific to Tamil and lacks broader applicability to other Indic languages. Sridhar et al. [13] used Hadoop-based LZW compression for large-scale medical texts. Gap: Focus is on scalability rather than language-specific optimization. Amir et al. [16] introduced an 8-bit transformation-based compression scheme for Bangla natural text using Unicode reduction. Gap: While highly effective for Bangla, it hasn’t been generalized to other Indic languages or integrated with hybrid strategies. Aslanyürek and Mesut [17] proposed WSDC, a static dictionary-based method enhanced by k-means clustering for short texts. Gap: Their study excludes Devanagari script and focuses on generic multilingual corpora. 
While the above mentioned studies provide valuable insights into hybrid compression techniques, there is a noticeable gap in research specifically targeting Hindi text compression. The unique characteristics of the Devanagari script, including its complex ligatures and diacritics, pose challenges that are not directly addressed by existing hybrid methods. This gap highlights the need for developing adaptive, customizable hybrid compression frameworks tailored to the linguistic and structural nuances of Hindi text.hybrid compression for Hindi remains an underexplored area.
3.METHODLOGY
A total of 75 algorithm–dataset combinations were evaluated in this study, encompassing five standard compression algorithms—LZMA, Zstd, Brotli, Bzip2, and LZ4HC—and their various hybrid configurations. Each algorithm or hybrid was applied to three UTF-8 encoded Hindi text datasets of different sizes (small, medium, and large) to facilitate a comprehensive comparative analysis.
3.1. Experimental Setup
The experiments were conducted on a system equipped with an AMD Ryzen 9 5900HX processor clocked at 3.30 GHz, with 16 GB DDR4 RAM, running Windows 11 Home (64-bit). All scripts and evaluations were performed using Python 3.11 (64-bit) within the IDLE environment.
3.2 Compression Datasets
Three UTF-8 encoded Hindi text datasets of varying sizes were used for evaluation. The small dataset is approximately 145 KB, the medium dataset around 1,600 KB, and the large dataset roughly 13,000 KB. These represent short-term, medium-term, and long-term text data scenarios, respectively.
3.3 Implementation Details
The compression framework employs a sequential hybrid approach, where two compression algorithms are applied in succession. Standard configurations were used for all algorithms: LZMA was set to level 6 with no dictionary; Zstd also used level 6 with no dictionary; Brotli was configured with quality level 6 and a window size of 22; Bzip2 was executed with its default settings and a block size of 900 KB; and LZ4HC was applied at level 6 without a dictionary.
3.4 Hybrid Compression-Decompression Workflow
To support the hybrid framework, each algorithm combination follows a two-step process during compression and decompression. The pseudocode below illustrates the simplified workflow applied uniformly across all algorithm–dataset combinations in this study











                          Algorithm 1.   Hybrid Compression and Decompression with Metric Calculation


3.5. Performance Metrics
3.5.1. Compression Ratio (CR)
The compression ratio is calculated by dividing the original size of the file by its compressed size:



This metric indicates how effectively an algorithm reduces the file size. A higher value signifies better compression efficiency.
3.5.2. Compression Speed (CS)
The compression speed is determined by dividing the compressed size by the time taken for compression		


Measured in megabytes per second (MB/s), this reflects how fast the algorithm compresses the data. Higher values denote faster performance.
3.4.3. Decompression Speed (DS)
The decompression speed is calculated by dividing the original size by the decompression time


Also in MB/s, this indicates the speed at which compressed data is restored. Higher values are desirable.
3.4.1.Formula and Efficiency Calculation
Compression efficiency (E) is evaluated using min-max normalized metrics:

where compression ratio (CR), compression speed (CS), and decompression speed (DS) are normalized as:
⁡ ⁡ 	
Normalization ensures fair comparison. The 40%-30%-30% weighting prioritizes compression ratio (40%) for better storage efficiency, while compression and decompression speeds (30% each) ensure computational feasibility. This balanced evaluation strategy aligns with previous comparative studies on lossless data compression algorithms that emphasize compression ratio, speed, decompression speed and efficiency as key performance indicators [16–19].
4. RESULT AND DISCUSSION
This section presents a detailed evaluation of the compression efficiency of six standard independent algorithms and their hybrid combinations (75 algorithms-dataset combinations) when applied to Hindi text files of varying sizes. The analysis is conducted on three datasets: small, medium, and large files. The results are benchmarked using four parameters: compression ratio (Equation 1), compression speed ( equation 2), decompression speed (Equation 3) and a derived efficiency metric. Efficiency was computed using a composite score: 40% compression ratio, 30% compression speed, and 30% decompression speed (Equation 4 and 5).

4.1. Compression Ratio
TABLE 1. Comparison of Independent and Hybrid Algorithm in terms of Compression  Ratio
Data set	Independent algorithm	Hybrid Algorithm
Small File	Bzip2 (6.57)	Bzip2+Brotli/Zstd  (6.57)
Medium File 	Bzip2 (7.56)	Bzip2+Brotli (7.6)
Large File	Brotli (117.11)	LZMA + brotli (141.59)
According to table 1 for small files, both the independent algorithm Bzip2 (6.57) and the hybrids Bzip2 + Brotli / Zstd (6.57) offered the highest compression ratio, showing that some hybrids can match the best-performing standalone compressors .For medium files, the hybrid Bzip2 + Brotli (7.6) marginally outperformed independent Bzip2 (7.56).For large files, the hybrid LZMA + Brotli (141.59) significantly exceeded Brotli(117.11) by offering 21% improvement in compression ratios, the best independent algorithm—demonstrating the effectiveness of deep-layered compression for larger datasets
4.2. Compression Speed
TABLE 2. Comparison of Independent and Hybrid Algorithm in terms of Compression  Speed
Data set	Independent algorithm
(Speed (MB/s))	Hybrid Algorithm
(Speed (MB/s))
Small File	Zstd (45.99)	Zstd + LZ4HC (46.31)
Medium File 	Zstd (76.61)	Zstd+Brotli (70.34)
Large File	Zstd (546.22)	Zstd+ Brotli (1071.57)
Table 2 shows that Zstd led among standalone algorithms for all file sizes.While, the hybrid Zstd + LZ4HC (46.31 MB/s) slightly exceeded Zstd (45.99 MB/s) for small files, while Zstd + Brotli (1071.57 MB/s) more than doubled Zstd's speed for large files, making it ideal for high-throughput compression applications
4.3. Decompression Speed
TABLE 3.Comparison of independent and hybrid algorithm in terms of  Decompression  Speed
Data set	Independent algorithm(Speed(MB/s))	Hybrid Algorithm(Speed(MB/s))
Small File	LZMA (281.19)	LZ4HC + Zstd (323.83)
Medium File 	LZMA (456.98)	LZ4HC + Zstd (485.04)
Large File	Zstd (593.79)	Zstd + LZ4HC (663.78)
Table 3 illustrates that for small and medium files, hybrid LZ4HC + Zstd outpaced LZMA, with decompression speeds of 323.83 MB/s and 485.04 MB/s, respectively.In large file decompression, Zstd + LZ4HC (663.78 MB/s) performed best, outperforming even the fastest standard decompressor (Zstd: 593.79 MB/s) by 11.78%.
4.4. Compression Efficiency
Efficiency was computed using a  normalized composite score: 40% compression ratio, 30% compression speed, and 30% decompression speed (Equation 4 and 5).

TABLE 4.Comparison of independent and hybrid algorithm in terms of  Compression Efficiency
Data set	Independent algorithm	Hybrid Algorithm
Small File	Zstd(0.5406)	Zstd + LZ4HC(0.6764)
Medium File 	 Zstd (0.6293)	 Zstd + Brotli(0.582)
Large File	Zstd (0.6836)	Zstd + LZ4HC(0.8567)

 From table 4 it is evident that results clearly highlight the dominance of hybrid algorithms, particularly combination like Zstd + LZ4HC in large files which outperform standard Zstd by nearly 25.7% and provides optimal compression without compromising speed.
5. STATISTICAL AND COMPARATIVE ANALYSIS


5.1. Trends in Compression Efficiency Across Dataset Sizes

Table 5. Top 10 Compression Algorithms Based on Weighted and Normalized Efficiency Scores      Small, Medium, and Large Hindi Text Datasets
Rank	Algorithm / Hybrid
(small files)	Efficiency
(small files)	Algorithm / Hybrid
(medium files)	Efficiency
(medium files)	Algorithm / Hybrid
(Large files)	Efficiency
(Large files)
1	Zstd + LZ4HC	0.6764	Zstd(Independent)	0.6293	Zstd + LZ4HC	0.8597
2	LZ4HC + Zstd	0.5708	Zstd + Brotli	0.582	Zstd + Brotli	0.8057
3	Bzip2 + LZ4HC	0.5591	Zstd + LZ4HC	0.5678	Zstd(Independent)	0.6836
4	Bzip2 + Zstd	0.5556	Bzip2 + Zstd	0.4908	Zstd + Bzip2	0.6779
5	Bzip2 + Brotli	0.5476	Bzip2 + Brotli	0.4907	Brotli + Zstd	0.6683
6	Brotli +Zstd	0.5430	Bzip2 + LZ4HC	0.4899	Zstd + LZMA	0.6539
7	Zstd (Independent)	0.5406	LZ4HC + Zstd	0.482	Brotli + LZ4HC	0.6116
8	Zstd + Brotli	0.4951	Bzip2(Independent)	0.4717	Brotli (Independent)	0.6073
9	LZMA(Independent)	0.4839	Bzip2 + LZMA	0.4584	Brotli + LZMA	0.5817
10	Bzip2 + LZMA	0.4788	Brotli + Zstd	0.462	Brotli + Bzip2	0.573

 
                                                                                      Efficiency 

Figure 1.  Normalized and Weighted Efficiency Comparison of Compression Techniques for Hindi Text

Table 5 and Figure 1 jointly illustrate the comparative efficiency of the top ten compression algorithms and hybrid methods across three dataset sizes—small, medium, and large. A consistent trend emerges wherein hybrid combinations significantly outperform most standalone algorithms, particularly for larger datasets. The hybrid method Zstd + LZ4HC secures the highest efficiency scores for both small (0.6764) and large (0.8597) files, while the standalone Zstd achieves the top rank for medium-sized files (0.6293). The bar chart in Figure 1 visually reinforces this pattern, highlighting the dominance of Zstd-centric hybrid configurations such as LZ4HC + Zstd, Bzip2 + Zstd, and Zstd + Brotli across various ranks and file sizes. Notably, the visual layout also emphasizes the increasing contribution of decompression speed (green bars) in the overall efficiency of top-ranking hybrids, particularly for larger datasets. This integrated analysis suggests that hybrid models leveraging Zstd as a core component consistently yield superior performance due to their ability to balance high compression ratio, fast compression speed, and efficient decompression. Consequently, Zstd + LZ4HC stands out as the most balanced and adaptable compression technique across diverse Hindi text data volumes.
To further validate the efficiency of the proposed hybrid algorithm, a direct performance comparison between Zstd + LZ4HC and the independent standard compression algorithms was conducted on the large file dataset. This comparison highlights the hybrid’s ability to maintain a high compression ratio while significantly improving speed, resulting in superior overall efficiency.
TABLE 6. Performance Comparison of the Proposed Hybrid Algorithm (Zstd + LZ4HC) with Standard Compression Algorithms on Large Hindi Text Files.
Algorithm	Compression Ratio	Compression Speed (MB/s)	Decompression Speed (MB/s)	Efficiency Score
Zstd + LZ4HC	94.82	1055.69	663.78	0.8597
Zstd	94.49	546.22	593.79	0.6836
Brotli	117.11	312.66	426.47	0.6073
Bzip2	9.77	18.75	4.88	0.021
LZ4HC	3.79	32.51	554.34	0.258
LZMA	141.91	5.96	16.56	0.0573


Table 6 presents this comparative analysis, showcasing the balanced advantage that Zstd + LZ4HC offers over all standard methods.. The hybrid achieves a strong efficiency score of 0.8597, combining a high compression ratio (94.82) with exceptional speed (1055.69 MB/s compression, 663.78 MB/s decompression). Among standard algorithms, Zstd performs best independently (efficiency: 0.6836) but remains outpaced by the hybrid. Others like LZMA and Bzip2, though offering high compression ratios, suffer from slow speeds, resulting in very low efficiency (0.0573 and 0.021, respectively). This concise comparison underscores the effectiveness of hybridization in balancing speed and compression quality for large-scale Hindi text compression.

5.3. Performance Balance Analysis (Compression Ratio vs. Compression Speed)

 We have analyze compression ratio vs. compression speed for large files, since that's where trade-offs are most pnounced
TABLE 8. Performance Balance Analysis

Algorithm	Compression Ratio	Compression Speed (MB/s)
LZMA + Brotli	141.59	6.09
LZMA + LZ4HC	141.10	6.01
Zstd + Brotli	94.49	1071.57
Zstd + LZ4HC	94.82	1055.69
Brotli + Zstd	117.10	491.50
Brotli + LZ4HC	117.08	426.05
Zstd (indep.)	94.59	546.22
Brotli + LZMA	117.5	208.44
	 FIGURE 2. Compression Ratio vs. Compression Speed Analysis

Table 8 and figure 2. analysis reveals that Zstd-based hybrids consistently achieve the best performance balance for large files. In particular, Zstd + Brotli and Zstd + LZ4HC stand out by offering exceptionally high compression speeds (over 1000 MB/s) while maintaining competitive compression ratios (~94.5). These are well-suited for real-time or high-throughput scenarios.
On the other hand, combinations like LZMA + Brotli and LZMA + LZ4HC deliver the highest compression ratios (above 141) but at significantly reduced speeds, making them ideal for storage-focused tasks where time is not critical.
Hybrid algorithms involving Brotli (e.g., Brotli + Zstd, Brotli + LZ4HC) emerge as strong all-rounders, balancing both compression ratio and speed effectively.
Overall, the results affirm that hybrid approaches can be tailored to specific needs, outperforming many standalone algorithms by strategically combining the strengths of two components.


5.4.  Contribution Frequency of Core Algorithms in Hybrid Compression Techniques

                 
FIGURE 3.  Frequency of Top Components in Top-Performing Hybrids. Total combinations analyzed: 30 (10 per size category)

Contribution Frequency of Core Algorithms in Hybrid Compression Techniques
Table 9 and figure 3 shows the component impact analysis of the frequency  in the top-performing hybrid compression algorithms reveals distinct trends in their relative contribution to overall efficiency. Among the 30 combinations evaluated across small, medium, and large Hindi text files, Zstd emerged as the most frequently occurring component, appearing in 15 of the top hybrids—accounting for 50% of all top-performing combinations. This was followed by Brotli, which appeared 11 times (36.7%), and Bzip2, included in 10 instances (33.3%). LZ4HC showed a solid presence with 8 appearances (26.7%), while LZMA appeared less frequently, in only 3 of the top hybrids (10%). These trends suggest that Zstd, Brotli, and Bzip2 are more likely to contribute to balanced and high-efficiency compression outcomes, making them strong candidates for constructing optimized hybrid algorithms. The dominance of Zstd in particular underscores its adaptability and performance when paired with other compressors.

6. CONCLUSION
This research highlights the remarkable potential of hybrid compression techniques for efficiently compressing Devanagari-encoded Hindi text, a script known for its complexity and richness. By evaluating a range of lossless algorithms, both independent and hybrid, across diverse datasets, this study uncovers key insights into how these methods perform under various compression and decompression conditions.
The results underscore a clear trend: hybrid algorithms consistently outperform their standard counterparts in terms of compression ratio, speed,decompression speed and overall performance. Among the various combinations tested, the hybrid pairing of Zstd + LZ4HC(Among top 3 across all the data sets) emerged as the standout performer, offering a 93.27% improvement in compression speed, 11.8% better decompression speed, and ~25% higher weighted and normalized efficiency compared to standard algorithms like Zstd in large dataset files. striking the optimal balance between compression ratio and processing time. This demonstrates that leveraging the strengths of multiple algorithms offers a clear advantage, particularly for large-scale text datasets.
These findings have profound implications for the digital preservation and transmission of Hindi text, as well as for developing applications in areas like machine learning, natural language processing, and web services. The results demonstrate that by optimizing compression for Devanagari-encoded Hindi, we can significantly improve the efficiency of handling large volumes of Hindi data in digital environments. Future research could focus on refining these hybrid models to further enhance performance, ensuring that such methods continue to evolve alongside the growing need for efficient multilingual data compression.
6.1. Real-World Applications 
The hybrid compression techniques presented in this study offer significant economic and societal benefits. By reducing the file sizes of Hindi text, these methods can lower storage costs and decrease the bandwidth needed for data transmission. This is especially valuable for digital libraries, archival systems, and online content delivery platforms, which often operate under budget and connectivity constraints. , the improved compression efficiency enhances accessibility to regional language content. Faster data transfer and reduced storage requirements can make digital educational resources, government documents, and cultural content more widely available, thereby supporting digital inclusion and literacy across diverse populations
6.2. Implications and Future Work
Although the current study demonstrates that hybrid algorithms—particularly the combination of Zstd + LZ4HC—consistently outperform standard methods in terms of compression ratio, speed, and overall efficiency, our findings also suggest that there is room for further optimization. In particular, the experimental results indicate that the performance of hybrid compression techniques varies with dataset size and text characteristics.
Based on these observations, we propose that future research should explore the development of an adaptive, customizable compression framework. Such a system would dynamically select or configure compression algorithms based on the specific properties of the Hindi text and the performance requirements of the application. By tailoring the compression strategy—whether to prioritize storage efficiency, processing speed, or a balanced trade-off—this adaptive framework could significantly enhance the practical application of compression techniques for Devanagari-encoded Hindi text. We believe that further investigation in this area could lead to a dedicated research study, which would build upon the findings of the current work.
.


References:
1. Y. Collet, Zstandard compression, available at https://github.com/facebook/zstd.
2. Google, Brotli, available at https://github.com/google/brotli/releases/tag/v0.2.0.
3. I. Pavlov, 7-Zip(2002), available at http://www.7-zip.org/.
4. J. Ziv and A. Lempel, “A universal algorithm for sequential data compression,” IEEE Trans. Inf. Theory 23, 337–343 (1977).
5. J.-L. Collet, LZ4 - Extremely Fast Compression, available at https://github.com/lz4/lz4.
6. J. Seward, The bzip2 program(1996), available at http://www.bzip.org/.
7. D. Salomon and G. Motta, Handbook of Data Compression, 5th ed. (Springer, London, 2010).
8. A. S. Nasif, Z. A. Othman, N. S. Sani, M. K. Hasan, and Y. Abudaqqa, “Huffman 0deep compression of edge node data for reducing IoT network traffic,” IEEE Access 12, 122988–122997 (2024).
9. A. Gupta, A. Bansal, and V. Khanduja, “Modern lossless compression techniques: Review, comparison and analysis,” in Proceedings of the 2017 International Conference on Electrical, Computer and Communication Technologies (ICECCT) (IEEE, Coimbatore, India, 2017), pp. 1–6.
10. J. Tariq, M. Farhan, and M. Abdulameer, “Improve compression data using serial combination techniques,” in 2022 8th International Conference on Contemporary Information Technology and Mathematics (ICCITM) (IEEE, Mosul, Iraq, 2022), pp. 290–293.
11. R. Gupta, A. Banerjee, and S. K. Mullick, “Coding of Devanagari composite character patterns for data compression,” IETE J. Res. 30, 168–172 (1984).
12. B. Vijayalakshmi and N. Sasirekha, “Lossless Tamil compression using ASCII substitution and modified Huffman encoding technique,” Int. J. Recent Technol. Eng. 8, 2900–2906 (2020).
13. P. Addepalli and P. V. Lakshmi, “An efficient lossless medical data compression using LZW compression for optimal cloud data storage,” Int. J. Health Sci. 25, 17144–17160 (2021).
14. M. Amaeer, K. M. A. Hasan, M. A. Mahmood, P. Shuvro, and A. Awal, “An efficient compression scheme for Bangla natural text by 16 bit Unicode transformation,” in Proceedings of the 2019 2nd International Conference on Innovation in Engineering and Technology (ICIET) (IEEE, Dhaka, Bangladesh, 2019), pp. 1–6.
15. M. Aslanyürek and A. Mesut, “A new method for short text compression,” IEEE Access 11, (2023).
16.  M. Al-Laham and I. M. M. El Emary, “Comparative study between various algorithms of data compression techniques,” in Proc. World Congr. Eng. Comput. Sci., 2007.
17. S. R. Kodituwakku and U. S. Amarasinghe, “Comparison of lossless data compression algorithm for text data,” Indian J. Comput. Sci. Eng. 1, 416–426 (2010).
18.  A. K. Bhattacharjee, T. Bej, and S. Agarwal, “Comparison study of lossless data compression algorithms for text data,” IOSR J. Comput. Eng. (IOSR-JCE) 11, 15–19 (2013).
19. S. Shanmugasundaram and R. Lourdusamy, “A comparative study of text compression algorithm,” Int. J. Wisdom Based Comput. 1, 3 (2011).












                                      
                                             






 

